hp:
  layers:
  - 2
  d_token:
  - 96
  - 192
  - 288
  lr:
  - 0.0003
  - 0.0001
  col_lr:
  - 0.0005
  - 0.001
  - 0.005
  attention_dropout:
  - 0.2
  - 0.0
  - 0.3


num_epochs: 100
patience: 20
